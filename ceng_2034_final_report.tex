\documentclass[onecolumn]{article}
%\usepackage{url}
%\usepackage{algorithmic}
\usepackage[a4paper]{geometry}
\usepackage{datetime}
\usepackage[margin=2em, font=small,labelfont=it]{caption}
\usepackage{graphicx}
\usepackage{mathpazo} % use palatino
\usepackage[defaultfam,tabular,lining]{montserrat}
\usepackage{microtype}
\usepackage{amsmath}
\usepackage{subfigure}

\usepackage{listings}
\usepackage{color}
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=python,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=2
}

% Letterspacing macros
\newcommand{\spacecaps}[1]{\textls[200]{\MakeUppercase{#1}}}
\newcommand{\spacesc}[1]{\textls[50]{\textsc{\MakeLowercase{#1}}}}

\title{\spacecaps{Multiprocessing, Hashes and HTTP Requests}\\ \normalsize \spacesc{CENG2034, Operating Systems} }

\author{Mehmet Pekcan\\mehmetpekcan@posta.mu.edu.tr\\\\\textbf{GitHub username}\\ mehmetpekcan\\\\\textbf{More detail check documentation}\\mehmetpekcan.gitbook.io/ceng2034}
%\date{\today\\\currenttime}
\date{\today}

\begin{document}
\maketitle

\begin{abstract}
Threading and multiprocessing always being a trouble in every cases. Both cases has benefits for their usage areas. In our case, we will implement a script to make HTTP request some URLs and download files. After downloading files, script will be identify them if there is a duplicate files. In this case, using multiprocessing is more beneficial for speed if the URLs count decreased insanely.
\end{abstract}


\section{Introduction}
Purpose of this report is learning how to use multiprocessing in our codes and how it will be increased our script performance. Meanwhile we will learn how to uniquely identify a file, how to make HTTP request, how to create child process and so on..

\section{Assignments}
In this report, duplicate image finding performed using multiprocessing and normal threading. And also which one is more efficient?

\subsection{What is used in the project\\}

\subsubsection{Which kernel and system version used in the project}
System Version: macOS 10.15.3 (19D76)
Kernel Version: Darwin 19.3.0

\subsubsection{Which language and version used in the project}
Python and version 3.8.0 were used.

\subsubsection{Which libraries used in the project}
\textbf{os}, for using operating systems command which is basically codes that uses in terminal\\
\textbf{time}, this for testing start and finish time to check which way is more efficient in speed way\\
\textbf{requests}, this is for making HTTP requests to given URLs\\
\textbf{uuid}, to generate a unique name\\
\textbf{hashlib}, to uniquely identify files if they are unique or duplicate\\
\textbf{multiprocessing}, this is for making multi processing at a time\\
\textbf{product}, to send two parameter to our mapping function (will be more details)\\
\textbf{re}, to use regex functionality in script\\

\subsection{Problems and How to solve them in the project\\}

\subsubsection{How to create child process and using it}
Creating child process in Python is so easy, after importing \textbf{os} library, we should first forking a process from parent. After forking child process, Python create a sub process of parent and when process id more than zero means that Python running at parent process, when process id is equal zero means that Python running at child process id. And also, if our parent process took less time compare to child process, it means that our script will be die before child process done. To prevent this we can say that process id to wait until child process done.\\

\begin{lstlisting}[language=Python, caption=Creating child process then writing its process id]
    def fork_childProcess_showID():
        childProcess = os.fork()
        if childProcess > 0:
            os.waitpid(childProcess, 0)
        elif childProcess == 0:
            print("Child process id is:", os.getpid())
\end{lstlisting}

\subsubsection{Sending Request to Download Files}
Sending HTTP request in Python can make using \textbf{request} library. We can make all HTTP request with this library such as GET, POST, PUT, DELETE.\\

\begin{lstlisting}[language=Python, caption=Downloading files]
def download_file(url, file_name=None):
    file_name = find_image_format(url, file_name)
    response = requests.get(url, allow_redirects=True)
    open(file_name, "wb").write(response.content)
\end{lstlisting}

\\Above code is making get request and setting it's name (we'll mention this later) after downloading, we're opening a file then writing request response content in it. Basically we are taking URLs file content, then opening a file and write in it to save this URLs content as a file our directory. And additionally find image format function is look like this:\\

\begin{lstlisting}[language=Python, caption=Finding image format then renaming it]
    def find_image_format(url, file_name):
        # if file has no parameter comes from user
        # generate an unique id
        file_name = file_name if file_name else str(uuid.uuid4())
        formats = ["jpeg", "jpg", "png", "gif", "tiff", "eps", "svg", "pdf"]
    
        # if any known files format has "url" find it
        # and suppose that this file is that format
        found_format = ""
        for extension in formats:
            for match in regex.finditer(extension, url):
                found_format = match.group()
    
        # return new file name with founded format of file
        return file_name+"."+found_format
\end{lstlisting}

\subsubsection{Creating Multiprocess}
Before continue with our task we should know to use multiprocesses. Multiprocesses run at a same time. Normally Python runs synchronously. So let's create a multiprocess using \textbf{multiprocessing} library.

\\Above code create a process pool to call function. Then mapping them with a function and parameter. And the output is:\\

\begin{lstlisting}[language=Python, caption=How to create multiprocess]
    import multiprocessing
    import time # this is for simulating
    
    def do_staff(second):
        print(f"do_staff function starts with parameter: {second}")
        time.sleep(second)
        print("do_staff function ends")
        
    if __name == "__main__":
        processPool = multiprocessing.Pool()
        processPool.map(do_staff, [1,2,3,4])
\end{lstlisting}

\subsubsection{Creating Multiprocess for our task}
First let's download files using \textbf{multiprocessing} library.\\

\begin{lstlisting}[language=Python, caption=Download File using Multiprocess]
    processPool = multiprocessing.Pool()
    
    # Assume that "...." end of the urls
    request_urls = [
    "http://wiki.netseclab.mu.edu.tr/images/thum.....",
    "https://upload.wikimedia.org/wikipedia/tr/9....",
    "https://upload.wikimedia.org/wikipedia/c....",
    "http://wiki.netseclab.mu.edu.tr/images/thum....",
    "https://upload.wikimedia.org/wikipedia/commons...."]
    
    processPool.map(download_file, request_urls)
\end{lstlisting}

\\Difference between normal way and multiprocessing way is that the way of calling methods in this case. Because our function was taking an URL then sending a request then writing into a file, nothing change when we want to use this function with multiprocessing on function. But calling is different. Let's look close above code, first we create a Pool to hold our multiprocess variable, after then we have URLs to send request. Using \textbf{Pool} variable, we use mapping function which is core function in Python. In this way, for every list item, our function will be work and all of them will be work asynchronously so all of them can be start at same time. Our first multiprocessing magic done successfully.\\

\\Time for implementing multiprocessing for duplicate image finding. We should know the way to handle this. Firstly we have to uniquely identify our image files. To do this we can use \textbf{md5} function in \textbf{hashlib}.\\

\begin{lstlisting}[language=Python, caption=Hash Generator Function]
    from hashlib import md5 
    
    def hashinize(image):
        with open(image, "rb") as imageFile:
            return imageHash = md5(imageFile.read()).hexdigest()
\end{lstlisting}

\\Above code takes image as a parameter then opens it as a file. After opening, it reads file in the \textbf{md5} function after reading it converts this value to 16 base system using \textbf{hexdigest} function. Then return it. And driver code for above code is this:\\

\begin{lstlisting}[language=Python, caption=Driver function at main]
        hashes = [ (hashinize(h), h) for h in cwdf ]
\end{lstlisting}

\\We downloaded our files and hashed them. Next thing is finding duplicate values? Take one image hash then check with the others, if equals it means that they are duplicate, if not it is unique.\\

\begin{lstlisting}[language=Python, caption=Function and driver code for duplicate image finding]
    def find_duplicates_by_multiproc(h, hashes):
        if h[0] == hashes[0]:
            return (h[1], hashes[1])

    duplicateRaw = processPool.starmap(find_duplicates_by_multiproc, product(hashes, repeat=2) )

    # This is just for filtering to duplicate tuples from their duplicate :)
    duplicateFiltered = []
    for i in duplicateRaw:
        if i != None:
            if i[0] != i[1] and i[1]:
                if i[0] not in duplicateFiltered:
                    duplicateFiltered.append(i[0])
                elif i[1] not in duplicateFiltered:
                    duplicateFiltered.append(i[1])
\end{lstlisting}

\\We used \textbf{starmap} above code. Because built-in \textbf{map} function is taking just one parameter to send function as a parameter, but in our case we should send two parameter to function. So we used \textbf{starmap} to send two parameter using \textbf{repeat} arguments.\\

\section{Results}

As a result, we found duplicate files in given directory. But we should see the big fish at the above explaining. It is about multiprocessing. Finding duplicate files using multiprocessing makes difference than normal threading. Let's see the difference in output.

\begin{lstlisting}[language=Python, caption=Output of difference]
    Elapsed time with serial threading is: 2.09sec
    Elapsed time with multiprocessing is: 1.02sec
\end{lstlisting}

\\As you see the result multiprocessing is running in a half time of serial threading. Think that you have thousands of data to making something, multiprocessing will be make huge different for you.

\section{Conclusion}

As a result, before starting project we have to determine to structure of project. We should test if i use multiprocessing will be more efficient or not? In many cases, multiprocessing make huge different in time.

\nocite{*}
\bibliographystyle{plain}
\bibliography{references}
\end{document}

